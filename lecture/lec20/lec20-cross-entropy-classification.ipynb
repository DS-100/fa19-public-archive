{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 20 Supplemental Notebook\n",
    "\n",
    "by Suraj Rampure, Josh Hug, John DeNero\n",
    "\n",
    "**Note:** In this lecture, the majority of the narrative and content is in the slides. The notebook is almost entirely supplementary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (5, 3)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "#plt.rcParams['lines.linewidth'] = 3\n",
    "sns.set()\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "import requests\n",
    "import os\n",
    "\n",
    "def fetch():\n",
    "    path = 'nba.csv'\n",
    "    if not os.path.exists(path):\n",
    "        url = 'https://stats.nba.com/stats/leaguegamelog/'\n",
    "        params = (\n",
    "            ('Counter', '0'),\n",
    "            ('DateFrom', ''),\n",
    "            ('DateTo', ''),\n",
    "            ('Direction', 'ASC'),\n",
    "            ('LeagueID', '00'),\n",
    "            ('PlayerOrTeam', 'T'),\n",
    "            ('Season', '2017-18'),\n",
    "            ('SeasonType', 'Regular Season'),\n",
    "            ('Sorter', 'DATE'),\n",
    "        )\n",
    "        headers = {\n",
    "            'User-Agent': 'PostmanRuntime/7.4.0'\n",
    "        }\n",
    "        response = requests.get(url, params=params, headers=headers)\n",
    "        data = response.json()['resultSets'][0]\n",
    "        df = pd.DataFrame(data=data['rowSet'], columns=data['headers'])\n",
    "        df.to_csv(path, index=False)\n",
    "        return df\n",
    "    else:\n",
    "        return pd.read_csv(path)\n",
    "    \n",
    "df = fetch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimizing Empirical Risk using Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Switching back to NBA data\n",
    "\n",
    "Let's start off with the cleaned dataframe from the end of last lecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_team = df.groupby(\"GAME_ID\").first()\n",
    "opponent = df.groupby(\"GAME_ID\").last()\n",
    "games = one_team.merge(opponent, left_index = True, right_index = True, suffixes = [\"\", \"_OPP\"])\n",
    "games[\"FG_PCT_DIFF\"] = games[\"FG_PCT\"] - games[\"FG_PCT_OPP\"]\n",
    "games['WON'] = games['WL'].replace('L', 0).replace('W', 1)\n",
    "games = games[['TEAM_NAME', 'MATCHUP', 'WON', 'FG_PCT_DIFF']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "games.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going any further, we need a $\\sigma(\\cdot)$ defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma(t):\n",
    "    return 1 / (1 + np.e**(-t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's try to predict `WON` from just `FG_PCT_DIFF`, as in last week's lecture. The below code determines the optimal $\\hat{\\beta}$ in this case, using MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss_single_arg_nba(beta):\n",
    "    x = games['FG_PCT_DIFF']\n",
    "    y_obs = games['WON']\n",
    "    y_hat = sigma(x * beta)\n",
    "    return np.mean((y_obs - y_hat)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_beta_mse_nba = minimize(mse_loss_single_arg_nba, x0 = 0)[\"x\"][0]\n",
    "best_beta_mse_nba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that this beta is the minimizing value of \n",
    "$$R(\\beta) = \\frac{1}{n} \\sum_{i = 1}^n (y_i - \\sigma(x_i\\beta))^2$$ where $x_i$ corresponds to `FG_PCT_DIFF` and $y_i$ corresponds to `WIN`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have $\\hat{\\beta}_{MSE}$, let's compute $\\hat{\\beta}_{CE}$. To do that, we'll create a `cross_entropy_loss` and `cross_entropy_loss_single_arg_nba`. We will pass the latter into `scipy.optimize.minimize`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining cross entropy loss (copied from above, repeated for clarity)\n",
    "def cross_entropy_loss(y, y_hat):\n",
    "    return -y * np.log(y_hat) - (1-y) * np.log(1-y_hat)\n",
    "\n",
    "def cross_entropy_loss_single_arg_nba(beta):\n",
    "    x = games['FG_PCT_DIFF']\n",
    "    y_obs = games['WON']\n",
    "    y_hat = sigma(beta * x)\n",
    "    return np.mean(cross_entropy_loss(y_obs, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_beta_ce_nba = minimize(cross_entropy_loss_single_arg_nba, x0 = 0)[\"x\"][0]\n",
    "print('optimal beta using mse on nba data: ', best_beta_mse_nba)\n",
    "print('optimal beta using mean cross entropy on nba data: ', best_beta_ce_nba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that $\\hat{\\beta}_{CE}$ is slightly greater than $\\hat{\\beta}_{MSE}$. That is, so long as our initial guess for $\\vec{\\beta}$ is in the right region, we're likely to end up with a $\\hat{\\beta}_{MSE}$ that's close to $\\hat{\\beta}_{CE}$. However, cross entropy loss is more likely to converge to a reasonable answer from a much wider range of initial guesses. (Due to other numerical issues, it's still possible that cross entropy loss doesn't actually converge. But, don't worry about that for our purposes.)\n",
    "\n",
    "Let's use $\\hat{\\beta}_{CE}$ to do some prediction. Let's overlay our fitted model over a scatterplot of `WON` vs `FG_PCT_DIFF`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg_pcts = np.linspace(-0.3, 0.3, 1000)\n",
    "y_pred = sigma(fg_pcts * best_beta_ce_nba)\n",
    "\n",
    "plt.xlabel('FG_PCT_DIFF')\n",
    "plt.ylabel(r'$P(Y = 1 | x)$')\n",
    "plt.plot(fg_pcts, y_pred, color = 'r')\n",
    "plt.scatter(games['FG_PCT_DIFF'], games['WON'] + np.random.normal(0, 0.01, len(games['WON'])));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's switch to a completely different dataset, and go through the entire process of fitting a model and using it to classify. We will be using `sklearn`'s built in `breast_cancer` dataset, a commonly used dataset for classification purposes. \n",
    "\n",
    "We will also split our data into train and test (as we always should). You will have an opportunity at the end of this notebook to test our model on the testing data, but we will not get to this point in lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = sklearn.datasets.load_breast_cancer()\n",
    "cancer = pd.DataFrame(data_dict['data'], columns=data_dict['feature_names'])\n",
    "cancer['bias'] = 1.0\n",
    "# Target data_dict['target'] = 0 is malignant; 1 is benign\n",
    "cancer['malignant'] = 1 - data_dict['target']\n",
    "cancer.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(cancer, test_size=0.25, random_state=100)\n",
    "print(\"Training Data Size: \", len(train))\n",
    "print(\"Test Data Size: \", len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we scatter `malignant` vs. `mean radius`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('mean radius')\n",
    "plt.ylabel('malignant')\n",
    "plt.scatter(train['mean radius'], train['malignant']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as we did in the last lecture, let's take a look at the average `malignant` value for various bins of `mean radius`. That's what we'll plot in gold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radii = np.linspace(5, 30, 50)\n",
    "averages = [np.average(train[np.abs(train['mean radius']-r)<2]['malignant']) for r in radii]\n",
    "plt.xlabel('mean radius')\n",
    "plt.ylabel('malignant')\n",
    "plt.scatter(train['mean radius'], train['malignant']);\n",
    "plt.scatter(radii, averages, color='gold');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features(t):\n",
    "    return t[['bias', 'mean radius']].values.T\n",
    "    \n",
    "x_train, y_train = features(train), train['malignant'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now use `sklearn.linear_model.LogisticRegression` to find the optimal $\\vec{\\hat{\\beta}}$ here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(fit_intercept=False, C=1e9, solver='lbfgs')\n",
    "model.fit(x_train.T, y_train)\n",
    "beta_2_features = model.coef_[0]\n",
    "beta_2_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this value of $\\vec{\\hat{\\beta}}$, let's plot of $P(Y = 1 | \\vec{x})$ in red, overlaid on our previous scatter plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(train['mean radius'], train['malignant'], label = 'original data');\n",
    "plt.xlabel('mean radius')\n",
    "plt.ylabel('malignant')\n",
    "plt.scatter(radii, averages, color='gold', label = 'binned means');\n",
    "plt.plot(radii, sigma(beta_2_features[0] + radii * beta_2_features[1]), color='r', label = 'logistic model');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now we've fit a model using two features (`mean radius` plus an intercept term). Let's now...\n",
    "1. Create a classifier (i.e. threshold our probabilities)\n",
    "2. Compute the accuracy, precision, and recall of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_prob(X, y):\n",
    "    model = LogisticRegression(fit_intercept = False, C=1e9, solver='lbfgs')\n",
    "    model.fit(X.T, y)\n",
    "    beta = model.coef_[0]\n",
    "    \n",
    "    return sigma(X.T @ beta)\n",
    "\n",
    "# Default threshold is 0.5, but we can change it\n",
    "def classify(vals, threshold = 0.5):\n",
    "    return np.int64(vals >= threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify(np.array(([0.2, 0.6, 0.4])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(actual, pred):\n",
    "    return np.mean(actual == pred)\n",
    "\n",
    "def precision(actual, pred):\n",
    "    # It's not necessary to define each of these in both the function for precision\n",
    "    # and recall, but they're here just for the sake of clarity\n",
    "    tp = sum((actual == pred) & (actual == 1))\n",
    "    tn = sum((actual == pred) & (actual == 0))\n",
    "    fp = sum((actual != pred) & (actual == 0))\n",
    "    fn = sum((actual != pred) & (actual == 1))\n",
    "    \n",
    "    return tp / (tp + fp)\n",
    "\n",
    "def recall(actual, pred):\n",
    "    tp = sum((actual == pred) & (actual == 1))\n",
    "    tn = sum((actual == pred) & (actual == 0))\n",
    "    fp = sum((actual != pred) & (actual == 0))\n",
    "    fn = sum((actual != pred) & (actual == 1))\n",
    "    \n",
    "    return tp / (tp + fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = predict_prob(x_train, y_train)\n",
    "y_pred = classify(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how these values change as we adjust our threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "for t in range(1, 11, 1):\n",
    "    threshold = t / 10\n",
    "    y_pred = classify(probs, threshold)\n",
    "    acc = accuracy(y_train, y_pred)\n",
    "    pre = precision(y_train, y_pred)\n",
    "    rec = recall(y_train, y_pred)\n",
    "    accuracies.append(acc)\n",
    "    precisions.append(pre)\n",
    "    recalls.append(rec)\n",
    "    print(\"threshold p >= {}: accuracy {}, precision {}, recall {}\".format(threshold, np.round(acc, 2),\n",
    "                                                                           np.round(pre, 2),\n",
    "                                                                           np.round(rec, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('precision')\n",
    "plt.ylabel('recall')\n",
    "plt.xlim(0.5, 1)\n",
    "plt.ylim(0.5, 1)\n",
    "plt.title('Precision vs. Recall')\n",
    "plt.scatter(precisions, recalls);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, as precision increases, recall decreases. This further emphasizes the point from lecture that there's a \"tradeoff\" between precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('threshold * 10')\n",
    "plt.ylabel('accuracy')\n",
    "plt.plot(list(range(1, 11, 1)), accuracies);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the otherhand, accuracy seems to peak near $t = 0.6$ as our threshold.\n",
    "\n",
    "**What we really should be doing is looking at testing accuracy, though, since we split our data into `train` and `test`.**\n",
    "We won't have time to get to this during lecture, but you should try and compute the testing accuracy, precision, and recall of our `malignant` classifier, and see how that changes with our threshold. (You will get some practice with these ideas in Project 2.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
